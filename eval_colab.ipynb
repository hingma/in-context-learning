{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Context Learning Model Evaluation\n",
        "\n",
        "Comprehensive evaluation notebook for trained transformer models on linear regression tasks.\n",
        "\n",
        "**Features:**\n",
        "- Load trained models from checkpoint\n",
        "- Full evaluation suite with multiple test scenarios\n",
        "- Baseline comparisons (OLS, k-NN, Averaging)\n",
        "- Visualization of in-context learning curves\n",
        "- Out-of-distribution robustness tests (random quadrants, orthogonal, scaling)\n",
        "- Works with qwen2.5, GPT-2, and LSTM models\n",
        "\n",
        "**Prerequisites:**\n",
        "- A trained model checkpoint (from train_colab.ipynb)\n",
        "- Model's run_id and output directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU and Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing packages...\\n\")\n",
        "\n",
        "# Core ML packages\n",
        "%pip install -q transformers>=4.30.0\n",
        "%pip install -q xgboost\n",
        "%pip install -q matplotlib seaborn tqdm pandas\n",
        "%pip install -q pyyaml\n",
        "%pip install -q munch\n",
        "%pip install -q scikit-learn\n",
        "\n",
        "# PyTorch usually comes pre-installed in Colab\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úì PyTorch already installed: {torch.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch...\")\n",
        "    %pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì All required packages installed successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verify key packages\n",
        "import torch\n",
        "import transformers\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(f\"\\nPackage Versions:\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "\n",
        "print(f\"\\nGPU Information:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  ‚ö†Ô∏è  No GPU detected! Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "print(\"\\n‚úì Ready for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mount Google Drive (if using Drive for storage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: If your model is stored in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/in-context-learning\n",
        "\n",
        "# Option 2: If using repository clone (uncomment if needed)\n",
        "# import os\n",
        "# import subprocess\n",
        "# REPO_URL = \"https://github.com/hingma/in-context-learning.git\"  # UPDATE THIS!\n",
        "# if not os.path.exists(\"in-context-learning\"):\n",
        "#     print(f\"Cloning repository from {REPO_URL}...\")\n",
        "#     subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n",
        "#     print(\"‚úì Repository cloned successfully\")\n",
        "# %cd in-context-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import Evaluation Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "# Import required modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from munch import Munch\n",
        "\n",
        "# Import project modules\n",
        "from eval import (\n",
        "    get_model_from_run, \n",
        "    get_run_metrics, \n",
        "    eval_model, \n",
        "    build_evals,\n",
        "    baseline_names\n",
        ")\n",
        "from models import build_model, get_relevant_baselines\n",
        "from tasks import get_task_sampler\n",
        "from samplers import get_data_sampler\n",
        "\n",
        "# Set plot style (matching eval.ipynb)\n",
        "sns.set_theme('notebook', 'darkgrid')\n",
        "palette = sns.color_palette('colorblind')\n",
        "\n",
        "print(\"‚úì All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Trained Model\n",
        "\n",
        "**IMPORTANT:** Update the `run_id` below with your trained model's run ID from training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# UPDATE THIS WITH YOUR RUN ID!\n",
        "# ========================================\n",
        "run_id = \"8a53116d-8c44-4687-9af4-bc8344eafbc7\"  # Replace with your run_id from training\n",
        "# ========================================\n",
        "\n",
        "run_path = os.path.join(\"./outputs\", run_id)\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(run_path):\n",
        "    raise FileNotFoundError(f\"Model not found at {run_path}. Please check your run_id!\")\n",
        "\n",
        "print(f\"Loading model from: {run_path}\\n\")\n",
        "\n",
        "# Load model and config\n",
        "model, conf = get_model_from_run(run_path, step=-1)  # step=-1 loads final checkpoint\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(f\"‚úì Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Running on CPU (slower)\")\n",
        "    \n",
        "model.eval()\n",
        "\n",
        "# Display model configuration\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Model Configuration:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model Family: {conf.model.family}\")\n",
        "print(f\"  Task: {conf.training.task}\")\n",
        "print(f\"  n_dims: {conf.model.n_dims}\")\n",
        "print(f\"  n_positions: {conf.model.n_positions}\")\n",
        "print(f\"  n_embd: {conf.model.n_embd}\")\n",
        "print(f\"  n_layer: {conf.model.n_layer}\")\n",
        "print(f\"  n_head: {conf.model.n_head}\")\n",
        "print(f\"  Training points: {conf.training.curriculum.points.end}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\n‚úì Model ready for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quick Test Evaluation\n",
        "\n",
        "Run a quick evaluation to verify the model works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup evaluation parameters from config\n",
        "n_dims = conf.model.n_dims\n",
        "n_points = conf.training.curriculum.points.end\n",
        "batch_size = 64\n",
        "task_name = conf.training.task\n",
        "data_name = conf.training.data\n",
        "\n",
        "# Create data and task samplers\n",
        "data_sampler = get_data_sampler(data_name, n_dims=n_dims)\n",
        "task_sampler = get_task_sampler(task_name, n_dims, batch_size)\n",
        "\n",
        "print(f\"Evaluation Setup:\")\n",
        "print(f\"  Task: {task_name}\")\n",
        "print(f\"  Data: {data_name}\")\n",
        "print(f\"  n_dims: {n_dims}\")\n",
        "print(f\"  n_points: {n_points}\")\n",
        "print(f\"  batch_size: {batch_size}\")\n",
        "\n",
        "# Generate test data\n",
        "task = task_sampler()\n",
        "xs = data_sampler.sample_xs(n_points, batch_size)\n",
        "ys = task.evaluate(xs)\n",
        "\n",
        "print(f\"\\nGenerated test data:\")\n",
        "print(f\"  xs shape: {xs.shape}  (batch_size, n_points, n_dims)\")\n",
        "print(f\"  ys shape: {ys.shape}  (batch_size, n_points)\")\n",
        "\n",
        "# Get model predictions\n",
        "device = \"cuda\" if torch.cuda.is_available() and model.name.split(\"_\")[0] in [\"gpt2\", \"qwen2.5\", \"lstm\"] else \"cpu\"\n",
        "with torch.no_grad():\n",
        "    pred = model(xs.to(device), ys.to(device))\n",
        "    pred = pred.cpu()\n",
        "\n",
        "print(f\"  pred shape: {pred.shape}\")\n",
        "\n",
        "# Compute loss\n",
        "metric = task.get_metric()\n",
        "loss = metric(pred, ys).numpy()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Quick Test Results:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Mean loss (all points): {loss.mean():.4f}\")\n",
        "print(f\"  First point mean loss: {loss[:, 0].mean():.4f}\")\n",
        "print(f\"  Final point mean loss: {loss[:, -1].mean():.4f}\")\n",
        "print(f\"  Baseline (zero estimator): {n_dims:.4f}\")\n",
        "print(f\"  Improvement: {(1 - loss[:, -1].mean() / n_dims) * 100:.1f}%\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\n‚úì Quick evaluation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize In-Context Learning Curve\n",
        "\n",
        "Visualize how the model's performance improves with more in-context examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning curve (matching eval.ipynb style)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Calculate mean\n",
        "mean_loss = loss.mean(axis=0)\n",
        "\n",
        "# Plot model performance\n",
        "plt.plot(mean_loss, lw=2, label=f\"{conf.model.family} ({conf.model.n_layer}L)\")\n",
        "\n",
        "# Add baseline\n",
        "plt.axhline(n_dims, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"# in-context examples\")\n",
        "plt.ylabel(\"squared error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Learning Curve Statistics:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Baseline (zero estimator): {n_dims:.4f}\")\n",
        "print(f\"Initial loss (1 example): {mean_loss[0]:.4f}\")\n",
        "print(f\"Final loss ({n_points} examples): {mean_loss[-1]:.4f}\")\n",
        "print(f\"Improvement over baseline: {(1 - mean_loss[-1] / n_dims) * 100:.1f}%\")\n",
        "print(f\"Total improvement (first to last): {(1 - mean_loss[-1] / mean_loss[0]) * 100:.1f}%\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Evaluation with Baselines\n",
        "\n",
        "Run full evaluation suite including baseline comparisons. This may take a few minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running comprehensive evaluation (this may take a few minutes)...\")\n",
        "print(\"\\nNote: This will compute metrics for:\")\n",
        "print(\"  - Your trained model\")\n",
        "print(\"  - Baseline methods (OLS, k-NN, Averaging)\")\n",
        "print(\"  - Multiple test scenarios (standard, random quadrants, orthogonal, scaling, etc.)\")\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "# This will cache results in metrics.json\n",
        "all_metrics = get_run_metrics(run_path, step=-1, cache=True, skip_model_load=False, skip_baselines=False)\n",
        "\n",
        "print(f\"\\n‚úì Comprehensive evaluation complete!\")\n",
        "print(f\"  Results cached in: {os.path.join(run_path, 'metrics.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare with Baseline Methods\n",
        "\n",
        "Visualize how your model compares to traditional baseline methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract standard evaluation results\n",
        "standard_metrics = all_metrics.get(\"standard\", {})\n",
        "\n",
        "if not standard_metrics:\n",
        "    print(\"No standard metrics found!\")\n",
        "else:\n",
        "    # Create comparison plot (matching eval.ipynb style)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    # Define model name\n",
        "    model_name = model.name\n",
        "    \n",
        "    # Plot model performance with colorblind-friendly palette\n",
        "    colors = sns.color_palette('colorblind')\n",
        "    \n",
        "    # Plot model performance\n",
        "    if model_name in standard_metrics:\n",
        "        means = standard_metrics[model_name][\"mean\"]\n",
        "        ax.plot(means, lw=2, label=f\"{conf.model.family} ({conf.model.n_layer}L)\", \n",
        "                color=colors[0])\n",
        "    \n",
        "    # Plot baseline models\n",
        "    baseline_keys = ['OLS_driver=None', 'averaging', 'NN_n=3_uniform']\n",
        "    baseline_labels_map = {\n",
        "        'OLS_driver=None': 'Least Squares',\n",
        "        'averaging': 'Averaging',\n",
        "        'NN_n=3_uniform': '3-NN',\n",
        "    }\n",
        "    \n",
        "    for idx, baseline_key in enumerate(baseline_keys, 1):\n",
        "        if baseline_key in standard_metrics:\n",
        "            means = standard_metrics[baseline_key][\"mean\"]\n",
        "            label = baseline_labels_map.get(baseline_key, baseline_key)\n",
        "            ax.plot(means, lw=2, label=label, linestyle='--', color=colors[idx])\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_xlabel(\"# in-context examples\")\n",
        "    ax.set_ylabel(\"squared error\")\n",
        "    ax.legend()\n",
        "    ax.set_ylim(bottom=0)\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Print comparison table\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PERFORMANCE COMPARISON (Final Point Loss)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Method':<40} {'Final Loss':<15} {'vs Baseline':<15}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "    \n",
        "    baseline_loss = n_dims  # Zero estimator\n",
        "    \n",
        "    for method_name, metrics in standard_metrics.items():\n",
        "        final_loss = metrics[\"mean\"][-1]\n",
        "        improvement = (1 - final_loss / baseline_loss) * 100\n",
        "        \n",
        "        # Clean up method name\n",
        "        if method_name == model_name:\n",
        "            display_name = f\"{conf.model.family.upper()} (Your Model)\"\n",
        "        else:\n",
        "            display_name = baseline_names(method_name)\n",
        "        \n",
        "        print(f\"{display_name:<40} {final_loss:<15.4f} {improvement:>+12.1f}%\")\n",
        "    \n",
        "    print(f\"{'-'*80}\")\n",
        "    print(f\"{'Baseline (Zero estimator)':<40} {baseline_loss:<15.4f} {'0.0%':>15}\")\n",
        "    print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Out-of-Distribution Robustness Tests\n",
        "\n",
        "Evaluate model performance on various out-of-distribution scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot OOD scenarios individually with data-driven axis limits\n",
        "# This creates separate plots for each scenario with appropriate scaling\n",
        "\n",
        "ood_scenarios = [\n",
        "    'random_quadrants',\n",
        "    'orthogonal_train_test',\n",
        "    'overlapping_train_test',\n",
        "    'half_subspace',\n",
        "    'skewed'\n",
        "]\n",
        "\n",
        "model_name = model.name\n",
        "colors = sns.color_palette('colorblind')\n",
        "\n",
        "# Baseline methods to include\n",
        "baseline_keys = ['OLS_driver=None', 'averaging', 'NN_n=3_uniform']\n",
        "baseline_labels_map = {\n",
        "    'OLS_driver=None': 'Least Squares',\n",
        "    'averaging': 'Averaging',\n",
        "    'NN_n=3_uniform': '3-Nearest Neighbors',\n",
        "}\n",
        "\n",
        "print(\"Plotting OOD scenarios (each with baseline comparisons):\\n\")\n",
        "\n",
        "for scenario in ood_scenarios:\n",
        "    if scenario not in all_metrics:\n",
        "        continue\n",
        "    \n",
        "    metric = all_metrics[scenario]\n",
        "    \n",
        "    # Determine scale factor for axis limits\n",
        "    if \"scale\" in scenario:\n",
        "        scale = float(scenario.split(\"=\")[-1])**2\n",
        "    else:\n",
        "        scale = 1.0\n",
        "    \n",
        "    # Create plot with adaptive size based on data range\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    \n",
        "    # Collect all values to determine proper axis limits\n",
        "    all_values = []\n",
        "    \n",
        "    # Plot model performance\n",
        "    if model_name in metric:\n",
        "        means = metric[model_name][\"mean\"]\n",
        "        ax.plot(means, lw=2, label=f\"{conf.model.family}\", color=colors[0])\n",
        "        all_values.extend(means)\n",
        "    \n",
        "    # Plot baselines\n",
        "    for idx, baseline_key in enumerate(baseline_keys, 1):\n",
        "        if baseline_key in metric:\n",
        "            means = metric[baseline_key][\"mean\"]\n",
        "            label = baseline_labels_map.get(baseline_key, baseline_key)\n",
        "            ax.plot(means, lw=2, label=label, linestyle='--', color=colors[idx])\n",
        "            all_values.extend(means)\n",
        "    \n",
        "    # Add trivial baseline\n",
        "    trivial = 1.0 * scale\n",
        "    ax.axhline(trivial, ls=\"--\", color=\"gray\", alpha=0.5)\n",
        "    \n",
        "    # Set title\n",
        "    ax.set_title(scenario, fontsize=12)\n",
        "    \n",
        "    # Set axis limits based on actual data\n",
        "    if all_values:\n",
        "        y_min = min(all_values)\n",
        "        y_max = max(all_values)\n",
        "        y_range = y_max - y_min\n",
        "        \n",
        "        # Add padding (10% on bottom, 15% on top for legend space)\n",
        "        ax.set_ylim(max(-0.1 * scale, y_min - 0.1 * y_range), \n",
        "                    y_max + 0.15 * y_range)\n",
        "    \n",
        "    # X-axis: zoom in for orthogonal tests (most interesting in first n_dims examples)\n",
        "    if \"ortho\" in scenario:\n",
        "        ax.set_xlim(-1, min(n_dims, len(means)) - 1)\n",
        "    else:\n",
        "        ax.set_xlim(-1, len(means))\n",
        "    \n",
        "    # Labels\n",
        "    ax.set_xlabel(\"in-context examples\", fontsize=11)\n",
        "    ax.set_ylabel(\"squared error\", fontsize=11)\n",
        "    ax.legend(loc=\"best\", fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"  ‚úì {scenario}: y-range [{y_min:.2f}, {y_max:.2f}]\")\n",
        "\n",
        "print(\"\\nNote: Axis limits are automatically adjusted based on actual data ranges.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Input/Output Scaling Tests\n",
        "\n",
        "Test model robustness to different input (x) and output (y) scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot scaling tests individually for better visibility\n",
        "scales = [0.333, 0.5, 2, 3]\n",
        "\n",
        "# Process and plot X-scaling\n",
        "print(\"\\nX-Scaling Robustness Tests:\")\n",
        "x_scaling_data = {}\n",
        "for scale in scales:\n",
        "    key = f\"scale-x={scale}\"\n",
        "    if key in all_metrics and model_name in all_metrics[key]:\n",
        "        x_scaling_data[scale] = {\n",
        "            'final': all_metrics[key][model_name][\"mean\"][-1],\n",
        "            'full': all_metrics[key][model_name][\"mean\"]\n",
        "        }\n",
        "\n",
        "# Process and plot Y-scaling  \n",
        "print(\"\\nY-Scaling Robustness Tests:\")\n",
        "y_scaling_data = {}\n",
        "for scale in scales:\n",
        "    key = f\"scale-y={scale}\"\n",
        "    if key in all_metrics and model_name in all_metrics[key]:\n",
        "        y_scaling_data[scale] = {\n",
        "            'final': all_metrics[key][model_name][\"mean\"][-1],\n",
        "            'full': all_metrics[key][model_name][\"mean\"]\n",
        "        }\n",
        "\n",
        "# Plot each scaling scenario separately with proper axis limits\n",
        "for dim, scaling_data in [('x', x_scaling_data), ('y', y_scaling_data)]:\n",
        "    if not scaling_data:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{dim.upper()}-Scaling scenarios:\")\n",
        "    \n",
        "    for scale, data in sorted(scaling_data.items()):\n",
        "        key = f\"scale-{dim}={scale}\"\n",
        "        if key not in all_metrics:\n",
        "            continue\n",
        "        \n",
        "        metric = all_metrics[key]\n",
        "        \n",
        "        # Create individual plot for each scale\n",
        "        fig, ax = plt.subplots(figsize=(7, 5))\n",
        "        \n",
        "        colors = sns.color_palette('colorblind')\n",
        "        all_values = []\n",
        "        \n",
        "        # Plot model\n",
        "        if model_name in metric:\n",
        "            means = metric[model_name][\"mean\"]\n",
        "            ax.plot(means, lw=2, label=f\"{conf.model.family}\", color=colors[0])\n",
        "            all_values.extend(means)\n",
        "        \n",
        "        # Plot baselines\n",
        "        for idx, baseline_key in enumerate(['OLS_driver=None', 'averaging', 'NN_n=3_uniform'], 1):\n",
        "            if baseline_key in metric:\n",
        "                means = metric[baseline_key][\"mean\"]\n",
        "                label = baseline_labels_map.get(baseline_key, baseline_key)\n",
        "                ax.plot(means, lw=2, label=label, linestyle='--', color=colors[idx])\n",
        "                all_values.extend(means)\n",
        "        \n",
        "        # Add trivial baseline (scaled)\n",
        "        trivial = scale**2 if dim == 'x' else scale**2\n",
        "        ax.axhline(trivial, ls=\"--\", color=\"gray\", alpha=0.5)\n",
        "        \n",
        "        # Set title\n",
        "        ax.set_title(f\"{key} (scale factor = {scale})\", fontsize=12)\n",
        "        \n",
        "        # Set axis limits based on data\n",
        "        if all_values:\n",
        "            y_min = min(all_values)\n",
        "            y_max = max(all_values)\n",
        "            y_range = y_max - y_min\n",
        "            ax.set_ylim(y_min - 0.1 * y_range, y_max + 0.15 * y_range)\n",
        "        \n",
        "        ax.set_xlim(-1, len(metric[model_name][\"mean\"]))\n",
        "        ax.set_xlabel(\"in-context examples\", fontsize=11)\n",
        "        ax.set_ylabel(\"squared error\", fontsize=11)\n",
        "        ax.legend(loc=\"best\", fontsize=9)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(f\"  ‚úì {key}: final loss = {data['final']:.2f}, y-range [{y_min:.2f}, {y_max:.2f}]\")\n",
        "\n",
        "# Summary plot: final losses vs scale factor\n",
        "if x_scaling_data or y_scaling_data:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # X-scaling summary\n",
        "    if x_scaling_data:\n",
        "        x_scales = sorted(x_scaling_data.keys())\n",
        "        x_losses = [x_scaling_data[s]['final'] for s in x_scales]\n",
        "        \n",
        "        ax1.plot(x_scales, x_losses, lw=2, marker='o', markersize=8, label='model')\n",
        "        if standard_loss:\n",
        "            ax1.axhline(standard_loss, color='gray', linestyle='--', \n",
        "                       label=f'standard: {standard_loss:.3f}')\n",
        "        \n",
        "        ax1.set_xlabel('input (x) scale factor', fontsize=12)\n",
        "        ax1.set_ylabel('final point squared error', fontsize=12)\n",
        "        ax1.set_title('X-Scaling: Final Point Loss', fontsize=13)\n",
        "        ax1.set_xscale('log')\n",
        "        ax1.legend(fontsize=10)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Y-scaling summary\n",
        "    if y_scaling_data:\n",
        "        y_scales = sorted(y_scaling_data.keys())\n",
        "        y_losses = [y_scaling_data[s]['final'] for s in y_scales]\n",
        "        \n",
        "        ax2.plot(y_scales, y_losses, lw=2, marker='o', markersize=8, label='model')\n",
        "        if standard_loss:\n",
        "            ax2.axhline(standard_loss, color='gray', linestyle='--', \n",
        "                       label=f'standard: {standard_loss:.3f}')\n",
        "        \n",
        "        ax2.set_xlabel('output (y) scale factor', fontsize=12)\n",
        "        ax2.set_ylabel('final point squared error', fontsize=12)\n",
        "        ax2.set_title('Y-Scaling: Final Point Loss', fontsize=13)\n",
        "        ax2.set_xscale('log')\n",
        "        ax2.legend(fontsize=10)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\n‚úì Scaling summary plots completed\")\n",
        "    \n",
        "    # Print scaling analysis\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SCALING ROBUSTNESS ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    if x_scaling_data:\n",
        "        print(f\"\\nInput (X) Scaling:\")\n",
        "        print(f\"{'  Scale':<12} {'Final Loss':<15} {'vs Standard':<15}\")\n",
        "        print(f\"  {'-'*42}\")\n",
        "        for scale in sorted(x_scaling_data.keys()):\n",
        "            loss = x_scaling_data[scale]['final']\n",
        "            diff = ((loss / standard_loss - 1) * 100) if standard_loss else 0\n",
        "            print(f\"  {scale:<12.3f} {loss:<15.4f} {diff:>+12.1f}%\")\n",
        "    \n",
        "    if y_scaling_data:\n",
        "        print(f\"\\nOutput (Y) Scaling:\")\n",
        "        print(f\"{'  Scale':<12} {'Final Loss':<15} {'vs Standard':<15}\")\n",
        "        print(f\"  {'-'*42}\")\n",
        "        for scale in sorted(y_scaling_data.keys()):\n",
        "            loss = y_scaling_data[scale]['final']\n",
        "            diff = ((loss / standard_loss - 1) * 100) if standard_loss else 0\n",
        "            print(f\"  {scale:<12.3f} {loss:<15.4f} {diff:>+12.1f}%\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No scaling test results found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Complete Evaluation Summary\n",
        "\n",
        "Generate a comprehensive summary of all evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"COMPLETE EVALUATION SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"\\nüìã MODEL INFORMATION:\")\n",
        "print(f\"  Run ID: {run_id}\")\n",
        "print(f\"  Model Family: {conf.model.family}\")\n",
        "print(f\"  Architecture: {conf.model.n_layer} layers, {conf.model.n_head} heads, {conf.model.n_embd} embedding dim\")\n",
        "print(f\"  Task: {conf.training.task}\")\n",
        "print(f\"  Training dimensions: {conf.model.n_dims}\")\n",
        "print(f\"  Context length: {conf.model.n_positions}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
        "if \"standard\" in all_metrics and model_name in all_metrics[\"standard\"]:\n",
        "    final_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "    standard_loss = final_loss\n",
        "    improvement = (1 - final_loss / n_dims) * 100\n",
        "    print(f\"  Standard (in-distribution) loss: {final_loss:.4f}\")\n",
        "    print(f\"  Improvement over baseline: {improvement:.1f}%\")\n",
        "else:\n",
        "    standard_loss = None\n",
        "\n",
        "print(f\"\\nüéØ BASELINE COMPARISONS:\")\n",
        "if \"standard\" in all_metrics:\n",
        "    for baseline_key in ['OLS_driver=None', 'averaging', 'NN_n=3_uniform']:\n",
        "        if baseline_key in all_metrics[\"standard\"]:\n",
        "            baseline_loss = all_metrics[\"standard\"][baseline_key][\"mean\"][-1]\n",
        "            baseline_name = baseline_names(baseline_key)\n",
        "            if model_name in all_metrics[\"standard\"]:\n",
        "                model_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "                advantage = ((baseline_loss - model_loss) / baseline_loss * 100)\n",
        "                symbol = \"‚úÖ\" if advantage > 0 else \"‚ö†Ô∏è\"\n",
        "                print(f\"  {symbol} vs {baseline_name}: {advantage:+.1f}% {'better' if advantage > 0 else 'worse'}\")\n",
        "\n",
        "print(f\"\\nüåê OUT-OF-DISTRIBUTION ROBUSTNESS:\")\n",
        "# Collect OOD results for summary\n",
        "ood_scenarios_summary = ['random_quadrants', 'orthogonal_train_test', 'overlapping_train_test', 'half_subspace', 'skewed']\n",
        "ood_results = {}\n",
        "for scenario in ood_scenarios_summary:\n",
        "    if scenario in all_metrics and model_name in all_metrics[scenario]:\n",
        "        ood_results[scenario] = all_metrics[scenario][model_name][\"mean\"][-1]\n",
        "\n",
        "if ood_results:\n",
        "    for scenario, loss in ood_results.items():\n",
        "        clean_name = scenario.replace('_', ' ').title()\n",
        "        if standard_loss:\n",
        "            degradation = ((loss / standard_loss - 1) * 100)\n",
        "            status = \"‚úÖ\" if abs(degradation) < 15 else \"‚ö†Ô∏è\" if abs(degradation) < 30 else \"‚ùå\"\n",
        "            print(f\"  {status} {clean_name}: {loss:.4f} ({degradation:+.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ {clean_name}: {loss:.4f}\")\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  SCALING ROBUSTNESS:\")\n",
        "# Collect scaling data for summary\n",
        "x_scaling_summary = {}\n",
        "y_scaling_summary = {}\n",
        "for scale in [0.333, 0.5, 2, 3]:\n",
        "    key_x = f\"scale-x={scale}\"\n",
        "    key_y = f\"scale-y={scale}\"\n",
        "    if key_x in all_metrics and model_name in all_metrics[key_x]:\n",
        "        x_scaling_summary[scale] = all_metrics[key_x][model_name][\"mean\"][-1]\n",
        "    if key_y in all_metrics and model_name in all_metrics[key_y]:\n",
        "        y_scaling_summary[scale] = all_metrics[key_y][model_name][\"mean\"][-1]\n",
        "\n",
        "if x_scaling_summary or y_scaling_summary:\n",
        "    if x_scaling_summary:\n",
        "        max_x_deg = max([abs((loss / standard_loss - 1) * 100) for loss in x_scaling_summary.values()]) if standard_loss else 0\n",
        "        print(f\"  Input (X) scaling: max degradation {max_x_deg:.1f}%\")\n",
        "    if y_scaling_summary:\n",
        "        max_y_deg = max([abs((loss / standard_loss - 1) * 100) for loss in y_scaling_summary.values()]) if standard_loss else 0\n",
        "        print(f\"  Output (Y) scaling: max degradation {max_y_deg:.1f}%\")\n",
        "\n",
        "print(f\"\\nüíæ SAVED RESULTS:\")\n",
        "print(f\"  Metrics file: {os.path.join(run_path, 'metrics.json')}\")\n",
        "print(f\"  Model checkpoint: {os.path.join(run_path, 'state.pt')}\")\n",
        "print(f\"  Configuration: {os.path.join(run_path, 'config.yaml')}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"‚úÖ EVALUATION COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nAll results have been cached and can be reloaded without recomputing.\")\n",
        "print(f\"To share results, download the metrics.json file from: {run_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Export/Download Results (Optional)\n",
        "\n",
        "Download evaluation results to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to download files when running on Colab\n",
        "# from google.colab import files\n",
        "\n",
        "# Download metrics\n",
        "# metrics_file = os.path.join(run_path, \"metrics.json\")\n",
        "# if os.path.exists(metrics_file):\n",
        "#     print(f\"Downloading {metrics_file}...\")\n",
        "#     files.download(metrics_file)\n",
        "#     print(\"‚úì Metrics downloaded\")\n",
        "\n",
        "# Download config\n",
        "# config_file = os.path.join(run_path, \"config.yaml\")\n",
        "# if os.path.exists(config_file):\n",
        "#     print(f\"Downloading {config_file}...\")\n",
        "#     files.download(config_file)\n",
        "#     print(\"‚úì Config downloaded\")\n",
        "\n",
        "# Create a summary report\n",
        "summary_file = os.path.join(run_path, \"evaluation_summary.txt\")\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EVALUATION SUMMARY\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(f\"Model: {conf.model.family}\\n\")\n",
        "    f.write(f\"Task: {conf.training.task}\\n\")\n",
        "    f.write(f\"Run ID: {run_id}\\n\\n\")\n",
        "    \n",
        "    if \"standard\" in all_metrics and model_name in all_metrics[\"standard\"]:\n",
        "        final_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "        f.write(f\"Standard Loss: {final_loss:.4f}\\n\")\n",
        "        f.write(f\"Improvement: {(1 - final_loss / n_dims) * 100:.1f}%\\n\\n\")\n",
        "    \n",
        "    f.write(\"Evaluation Scenarios:\\n\")\n",
        "    for scenario in all_metrics.keys():\n",
        "        if model_name in all_metrics[scenario]:\n",
        "            loss = all_metrics[scenario][model_name][\"mean\"][-1]\n",
        "            f.write(f\"  - {scenario}: {loss:.4f}\\n\")\n",
        "\n",
        "print(f\"\\n‚úì Summary saved to: {summary_file}\")\n",
        "\n",
        "# Uncomment to download summary\n",
        "# files.download(summary_file)\n",
        "\n",
        "print(\"\\nüí° Tip: Run this notebook anytime to re-evaluate or visualize cached results!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Custom Robustness Test Example\n",
        "\n",
        "Example: Test model robustness to input scaling (similar to eval.ipynb).\n",
        "\n",
        "As an exploration, you can test how robust the model is to scaling inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with doubled inputs\n",
        "xs2 = 2 * xs\n",
        "ys2 = task.evaluate(xs2)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred2 = model(xs2.to(device), ys2.to(device))\n",
        "    pred2 = pred2.cpu()\n",
        "\n",
        "loss2 = metric(pred2, ys2).numpy()\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss.mean(axis=0), lw=2, label=\"standard inputs\")\n",
        "plt.plot(loss2.mean(axis=0) / 4, lw=2, label=\"doubled inputs (scaled)\")\n",
        "plt.axhline(n_dims, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
        "plt.xlabel(\"# in-context examples\")\n",
        "plt.ylabel(\"squared error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThe error may increase with doubled inputs, especially when the number\")\n",
        "print(\"of in-context examples exceeds the dimension, but the model should\")\n",
        "print(\"still remain relatively accurate.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Notes\n",
        "\n",
        "**Advanced Plotting with Data-Driven Axis Limits**: This notebook features intelligent visualization:\n",
        "- **Seaborn 'notebook' theme** with 'darkgrid' style and colorblind-friendly palette\n",
        "- **Data-driven axis limits**: Each plot automatically adjusts its y-axis based on actual value ranges\n",
        "- **Individual scenario plots**: Each test scenario (OOD, scaling) gets its own optimized plot\n",
        "- **Smart zooming**: \n",
        "  - Orthogonal tests focus on the first `n_dims` examples (most informative region)\n",
        "  - Scaling tests show full learning curves with proper scale adjustments\n",
        "  - All plots ensure curves are fully visible without excessive whitespace\n",
        "\n",
        "**Key Features**:\n",
        "1. **Cached Results**: All metrics are saved in `metrics.json` and can be reloaded without recomputation\n",
        "2. **Comprehensive Tests**: Standard, OOD (with adaptive views), and scaling robustness evaluations  \n",
        "3. **Baseline Comparisons**: Every plot includes OLS, k-NN, and averaging baselines\n",
        "4. **Adaptive Visualization**: Handles diverse value ranges (from 0.2 to 80+) automatically\n",
        "5. **Custom Tests**: Easy to add your own robustness tests (see doubled inputs example)\n",
        "\n",
        "**Understanding the Plots**:\n",
        "- **Standard evaluation**: Shows in-context learning across all examples\n",
        "- **OOD scenarios**: Individual plots with data-driven axis limits, printed y-ranges for reference\n",
        "- **Scaling tests**: Both individual learning curves AND summary plots comparing final losses\n",
        "- **Gray dashed line**: Shows the trivial baseline (adjusted for scaling scenarios)\n",
        "- **Value ranges**: Automatically printed for each scenario to understand data scale\n",
        "\n",
        "**Plot Optimization Examples**:\n",
        "- `random_quadrants`: y-range [~9, ~20] - model doesn't converge as well\n",
        "- `half_subspace`: y-range [~0.5, ~20] - excellent convergence\n",
        "- `scale-x=2`: y-range [~4, ~83] - scaled values, still shows convergence pattern\n",
        "- Each plot adapts to show the full story without clipping\n",
        "\n",
        "**Next Steps**:\n",
        "- Compare different model architectures (qwen2.5 vs GPT-2)\n",
        "- Test on different tasks (sparse linear regression, decision trees, etc.)\n",
        "- Experiment with different training curricula\n",
        "- Analyze learned representations or attention patterns\n",
        "- Use printed y-ranges to quickly identify problematic scenarios"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Context Learning Model Evaluation\n",
        "\n",
        "Comprehensive evaluation notebook for trained transformer models on linear regression tasks.\n",
        "\n",
        "**Features:**\n",
        "- Load trained models from checkpoint\n",
        "- Full evaluation suite with multiple test scenarios\n",
        "- Baseline comparisons (OLS, k-NN, Averaging)\n",
        "- Visualization of in-context learning curves\n",
        "- Out-of-distribution robustness tests (random quadrants, orthogonal, scaling)\n",
        "- Works with qwen2.5, GPT-2, and LSTM models\n",
        "\n",
        "**Prerequisites:**\n",
        "- A trained model checkpoint (from train_colab.ipynb)\n",
        "- Model's run_id and output directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU and Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Installing packages...\\n\")\n",
        "\n",
        "# Core ML packages\n",
        "%pip install -q transformers>=4.30.0\n",
        "%pip install -q xgboost\n",
        "%pip install -q matplotlib seaborn tqdm pandas\n",
        "%pip install -q pyyaml\n",
        "%pip install -q munch\n",
        "%pip install -q scikit-learn\n",
        "\n",
        "# PyTorch usually comes pre-installed in Colab\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úì PyTorch already installed: {torch.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch...\")\n",
        "    %pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì All required packages installed successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verify key packages\n",
        "import torch\n",
        "import transformers\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(f\"\\nPackage Versions:\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "\n",
        "print(f\"\\nGPU Information:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  ‚ö†Ô∏è  No GPU detected! Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "print(\"\\n‚úì Ready for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mount Google Drive (if using Drive for storage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: If your model is stored in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/in-context-learning\n",
        "\n",
        "# Option 2: If using repository clone (uncomment if needed)\n",
        "# import os\n",
        "# import subprocess\n",
        "# REPO_URL = \"https://github.com/hingma/in-context-learning.git\"  # UPDATE THIS!\n",
        "# if not os.path.exists(\"in-context-learning\"):\n",
        "#     print(f\"Cloning repository from {REPO_URL}...\")\n",
        "#     subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n",
        "#     print(\"‚úì Repository cloned successfully\")\n",
        "# %cd in-context-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import Evaluation Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "# Import required modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from munch import Munch\n",
        "\n",
        "# Import project modules\n",
        "from eval import (\n",
        "    get_model_from_run, \n",
        "    get_run_metrics, \n",
        "    eval_model, \n",
        "    build_evals,\n",
        "    baseline_names\n",
        ")\n",
        "from models import build_model, get_relevant_baselines\n",
        "from tasks import get_task_sampler\n",
        "from samplers import get_data_sampler\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úì All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Trained Model\n",
        "\n",
        "**IMPORTANT:** Update the `run_id` below with your trained model's run ID from training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# UPDATE THIS WITH YOUR RUN ID!\n",
        "# ========================================\n",
        "run_id = \"8a53116d-8c44-4687-9af4-bc8344eafbc7\"  # Replace with your run_id from training\n",
        "# ========================================\n",
        "\n",
        "run_path = os.path.join(\"./outputs\", run_id)\n",
        "\n",
        "# Check if model exists\n",
        "if not os.path.exists(run_path):\n",
        "    raise FileNotFoundError(f\"Model not found at {run_path}. Please check your run_id!\")\n",
        "\n",
        "print(f\"Loading model from: {run_path}\\n\")\n",
        "\n",
        "# Load model and config\n",
        "model, conf = get_model_from_run(run_path, step=-1)  # step=-1 loads final checkpoint\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(f\"‚úì Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Running on CPU (slower)\")\n",
        "    \n",
        "model.eval()\n",
        "\n",
        "# Display model configuration\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Model Configuration:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Model Family: {conf.model.family}\")\n",
        "print(f\"  Task: {conf.training.task}\")\n",
        "print(f\"  n_dims: {conf.model.n_dims}\")\n",
        "print(f\"  n_positions: {conf.model.n_positions}\")\n",
        "print(f\"  n_embd: {conf.model.n_embd}\")\n",
        "print(f\"  n_layer: {conf.model.n_layer}\")\n",
        "print(f\"  n_head: {conf.model.n_head}\")\n",
        "print(f\"  Training points: {conf.training.curriculum.points.end}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\n‚úì Model ready for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quick Test Evaluation\n",
        "\n",
        "Run a quick evaluation to verify the model works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup evaluation parameters from config\n",
        "n_dims = conf.model.n_dims\n",
        "n_points = conf.training.curriculum.points.end\n",
        "batch_size = 64\n",
        "task_name = conf.training.task\n",
        "data_name = conf.training.data\n",
        "\n",
        "# Create data and task samplers\n",
        "data_sampler = get_data_sampler(data_name, n_dims=n_dims)\n",
        "task_sampler = get_task_sampler(task_name, n_dims, batch_size)\n",
        "\n",
        "print(f\"Evaluation Setup:\")\n",
        "print(f\"  Task: {task_name}\")\n",
        "print(f\"  Data: {data_name}\")\n",
        "print(f\"  n_dims: {n_dims}\")\n",
        "print(f\"  n_points: {n_points}\")\n",
        "print(f\"  batch_size: {batch_size}\")\n",
        "\n",
        "# Generate test data\n",
        "task = task_sampler()\n",
        "xs = data_sampler.sample_xs(n_points, batch_size)\n",
        "ys = task.evaluate(xs)\n",
        "\n",
        "print(f\"\\nGenerated test data:\")\n",
        "print(f\"  xs shape: {xs.shape}  (batch_size, n_points, n_dims)\")\n",
        "print(f\"  ys shape: {ys.shape}  (batch_size, n_points)\")\n",
        "\n",
        "# Get model predictions\n",
        "device = \"cuda\" if torch.cuda.is_available() and model.name.split(\"_\")[0] in [\"gpt2\", \"qwen2.5\", \"lstm\"] else \"cpu\"\n",
        "with torch.no_grad():\n",
        "    pred = model(xs.to(device), ys.to(device))\n",
        "    pred = pred.cpu()\n",
        "\n",
        "print(f\"  pred shape: {pred.shape}\")\n",
        "\n",
        "# Compute loss\n",
        "metric = task.get_metric()\n",
        "loss = metric(pred, ys).numpy()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Quick Test Results:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Mean loss (all points): {loss.mean():.4f}\")\n",
        "print(f\"  First point mean loss: {loss[:, 0].mean():.4f}\")\n",
        "print(f\"  Final point mean loss: {loss[:, -1].mean():.4f}\")\n",
        "print(f\"  Baseline (zero estimator): {n_dims:.4f}\")\n",
        "print(f\"  Improvement: {(1 - loss[:, -1].mean() / n_dims) * 100:.1f}%\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\n‚úì Quick evaluation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize In-Context Learning Curve\n",
        "\n",
        "Visualize how the model's performance improves with more in-context examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning curve\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Calculate mean and std\n",
        "mean_loss = loss.mean(axis=0)\n",
        "std_loss = loss.std(axis=0)\n",
        "\n",
        "# Plot model performance\n",
        "plt.plot(range(n_points), mean_loss, lw=2.5, \n",
        "         label=f\"{conf.model.family} ({conf.model.n_layer}L)\", \n",
        "         marker='o', markersize=4, color='#2E86AB')\n",
        "\n",
        "# Add confidence interval\n",
        "plt.fill_between(range(n_points), \n",
        "                 mean_loss - std_loss, \n",
        "                 mean_loss + std_loss, \n",
        "                 alpha=0.2, color='#2E86AB')\n",
        "\n",
        "# Add baseline\n",
        "plt.axhline(n_dims, ls=\"--\", color=\"gray\", lw=2, label=\"Zero estimator baseline\")\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"Number of in-context examples\", fontsize=13, fontweight='bold')\n",
        "plt.ylabel(\"Squared Error\", fontsize=13, fontweight='bold')\n",
        "plt.title(f\"In-Context Learning Performance: {task_name}\", fontsize=15, fontweight='bold')\n",
        "plt.legend(fontsize=11, loc='upper right')\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Learning Curve Statistics:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Baseline (zero estimator): {n_dims:.4f}\")\n",
        "print(f\"Initial loss (1 example): {mean_loss[0]:.4f}\")\n",
        "print(f\"Final loss ({n_points} examples): {mean_loss[-1]:.4f}\")\n",
        "print(f\"Improvement over baseline: {(1 - mean_loss[-1] / n_dims) * 100:.1f}%\")\n",
        "print(f\"Total improvement (first to last): {(1 - mean_loss[-1] / mean_loss[0]) * 100:.1f}%\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comprehensive Evaluation with Baselines\n",
        "\n",
        "Run full evaluation suite including baseline comparisons. This may take a few minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running comprehensive evaluation (this may take a few minutes)...\")\n",
        "print(\"\\nNote: This will compute metrics for:\")\n",
        "print(\"  - Your trained model\")\n",
        "print(\"  - Baseline methods (OLS, k-NN, Averaging)\")\n",
        "print(\"  - Multiple test scenarios (standard, random quadrants, orthogonal, scaling, etc.)\")\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "# This will cache results in metrics.json\n",
        "all_metrics = get_run_metrics(run_path, step=-1, cache=True, skip_model_load=False, skip_baselines=False)\n",
        "\n",
        "print(f\"\\n‚úì Comprehensive evaluation complete!\")\n",
        "print(f\"  Results cached in: {os.path.join(run_path, 'metrics.json')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare with Baseline Methods\n",
        "\n",
        "Visualize how your model compares to traditional baseline methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract standard evaluation results\n",
        "standard_metrics = all_metrics.get(\"standard\", {})\n",
        "\n",
        "if not standard_metrics:\n",
        "    print(\"No standard metrics found!\")\n",
        "else:\n",
        "    # Create comparison plot\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    \n",
        "    # Define model name\n",
        "    model_name = model.name\n",
        "    \n",
        "    # Plot model performance\n",
        "    if model_name in standard_metrics:\n",
        "        means = standard_metrics[model_name][\"mean\"]\n",
        "        stds = np.array(standard_metrics[model_name][\"std\"])\n",
        "        \n",
        "        ax.plot(means, lw=3, label=f\"üöÄ {conf.model.family.upper()} (Ours)\", \n",
        "                marker='o', markersize=6, color='#E63946', zorder=10)\n",
        "        ax.fill_between(range(len(means)), \n",
        "                        np.array(means) - stds, \n",
        "                        np.array(means) + stds, \n",
        "                        alpha=0.2, color='#E63946')\n",
        "    \n",
        "    # Plot baseline models\n",
        "    baseline_colors = {\n",
        "        'OLS_driver=None': '#457B9D',\n",
        "        'averaging': '#F4A261',\n",
        "        'NN_n=3_uniform': '#2A9D8F',\n",
        "    }\n",
        "    \n",
        "    baseline_labels = {\n",
        "        'OLS_driver=None': 'Least Squares',\n",
        "        'averaging': 'Averaging',\n",
        "        'NN_n=3_uniform': '3-Nearest Neighbors',\n",
        "    }\n",
        "    \n",
        "    for baseline_key, color in baseline_colors.items():\n",
        "        if baseline_key in standard_metrics:\n",
        "            means = standard_metrics[baseline_key][\"mean\"]\n",
        "            label = baseline_labels.get(baseline_key, baseline_key)\n",
        "            ax.plot(means, lw=2, label=f\"üìä {label}\", \n",
        "                   linestyle='--', marker='s', markersize=4, \n",
        "                   alpha=0.8, color=color)\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_xlabel(\"Number of in-context examples\", fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel(\"Squared Error\", fontsize=13, fontweight='bold')\n",
        "    ax.set_title(\"Model vs Baseline Methods: Standard Evaluation\", fontsize=15, fontweight='bold')\n",
        "    ax.legend(fontsize=11, loc='upper right', framealpha=0.95)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(bottom=0)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print comparison table\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PERFORMANCE COMPARISON (Final Point Loss)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"{'Method':<40} {'Final Loss':<15} {'vs Baseline':<15}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "    \n",
        "    baseline_loss = n_dims  # Zero estimator\n",
        "    \n",
        "    for method_name, metrics in standard_metrics.items():\n",
        "        final_loss = metrics[\"mean\"][-1]\n",
        "        improvement = (1 - final_loss / baseline_loss) * 100\n",
        "        \n",
        "        # Clean up method name\n",
        "        if method_name == model_name:\n",
        "            display_name = f\"üöÄ {conf.model.family.upper()} (Your Model)\"\n",
        "        else:\n",
        "            display_name = f\"üìä {baseline_names(method_name)}\"\n",
        "        \n",
        "        print(f\"{display_name:<40} {final_loss:<15.4f} {improvement:>+12.1f}%\")\n",
        "    \n",
        "    print(f\"{'-'*80}\")\n",
        "    print(f\"{'Baseline (Zero estimator)':<40} {baseline_loss:<15.4f} {'0.0%':>15}\")\n",
        "    print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Out-of-Distribution Robustness Tests\n",
        "\n",
        "Evaluate model performance on various out-of-distribution scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify OOD test scenarios from the evaluation results\n",
        "ood_scenarios = [\n",
        "    'random_quadrants',\n",
        "    'orthogonal_train_test',\n",
        "    'overlapping_train_test',\n",
        "    'half_subspace',\n",
        "    'skewed'\n",
        "]\n",
        "\n",
        "# Collect OOD results for the model\n",
        "ood_results = {}\n",
        "model_name = model.name\n",
        "\n",
        "for scenario in ood_scenarios:\n",
        "    if scenario in all_metrics and model_name in all_metrics[scenario]:\n",
        "        ood_results[scenario] = all_metrics[scenario][model_name][\"mean\"][-1]\n",
        "\n",
        "# Also get standard performance for comparison\n",
        "if \"standard\" in all_metrics and model_name in all_metrics[\"standard\"]:\n",
        "    standard_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "else:\n",
        "    standard_loss = None\n",
        "\n",
        "# Create visualization\n",
        "if ood_results:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Bar plot of OOD performance\n",
        "    scenarios = list(ood_results.keys())\n",
        "    losses = list(ood_results.values())\n",
        "    \n",
        "    colors = ['#E63946' if l < standard_loss else '#F4A261' for l in losses]\n",
        "    \n",
        "    ax1.barh(scenarios, losses, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    if standard_loss:\n",
        "        ax1.axvline(standard_loss, color='#457B9D', linestyle='--', linewidth=2, \n",
        "                   label=f'Standard (in-distribution): {standard_loss:.3f}')\n",
        "    ax1.set_xlabel('Final Point Squared Error', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Out-of-Distribution Robustness', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Relative performance table\n",
        "    ax2.axis('off')\n",
        "    \n",
        "    table_data = []\n",
        "    table_data.append(['Scenario', 'Final Loss', 'vs Standard'])\n",
        "    table_data.append(['-'*30, '-'*12, '-'*12])\n",
        "    \n",
        "    if standard_loss:\n",
        "        table_data.append(['Standard (in-dist)', f'{standard_loss:.4f}', '‚Äî'])\n",
        "    \n",
        "    for scenario, loss in ood_results.items():\n",
        "        if standard_loss:\n",
        "            relative = ((loss / standard_loss - 1) * 100)\n",
        "            rel_str = f'{relative:+.1f}%'\n",
        "        else:\n",
        "            rel_str = '‚Äî'\n",
        "        \n",
        "        # Clean scenario name\n",
        "        clean_name = scenario.replace('_', ' ').title()\n",
        "        table_data.append([clean_name, f'{loss:.4f}', rel_str])\n",
        "    \n",
        "    # Draw table\n",
        "    table = ax2.table(cellText=table_data, \n",
        "                     cellLoc='left',\n",
        "                     loc='center',\n",
        "                     colWidths=[0.5, 0.25, 0.25])\n",
        "    \n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2)\n",
        "    \n",
        "    # Style header\n",
        "    for i in range(3):\n",
        "        table[(0, i)].set_facecolor('#457B9D')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    # Style standard row\n",
        "    if standard_loss:\n",
        "        for i in range(3):\n",
        "            table[(2, i)].set_facecolor('#E8F4F8')\n",
        "            table[(2, i)].set_text_props(weight='bold')\n",
        "    \n",
        "    ax2.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed analysis\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"OUT-OF-DISTRIBUTION ROBUSTNESS ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\n‚úì Model shows \", end=\"\")\n",
        "    \n",
        "    if standard_loss:\n",
        "        avg_degradation = np.mean([abs(l - standard_loss) / standard_loss * 100 for l in losses])\n",
        "        if avg_degradation < 10:\n",
        "            print(f\"EXCELLENT robustness (avg degradation: {avg_degradation:.1f}%)\")\n",
        "        elif avg_degradation < 25:\n",
        "            print(f\"GOOD robustness (avg degradation: {avg_degradation:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"MODERATE robustness (avg degradation: {avg_degradation:.1f}%)\")\n",
        "    else:\n",
        "        print(\"results on OOD scenarios\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No OOD evaluation results found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Input/Output Scaling Tests\n",
        "\n",
        "Test model robustness to different input (x) and output (y) scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract scaling test results\n",
        "scaling_tests = {}\n",
        "scales = [0.333, 0.5, 2, 3]\n",
        "\n",
        "for dim in ['x', 'y']:\n",
        "    scaling_tests[dim] = {}\n",
        "    for scale in scales:\n",
        "        key = f\"scale-{dim}={scale}\"\n",
        "        if key in all_metrics and model_name in all_metrics[key]:\n",
        "            scaling_tests[dim][scale] = all_metrics[key][model_name][\"mean\"][-1]\n",
        "\n",
        "# Visualize scaling robustness\n",
        "if scaling_tests['x'] or scaling_tests['y']:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # X-scaling\n",
        "    if scaling_tests['x']:\n",
        "        x_scales = sorted(scaling_tests['x'].keys())\n",
        "        x_losses = [scaling_tests['x'][s] for s in x_scales]\n",
        "        \n",
        "        ax1.plot(x_scales, x_losses, marker='o', markersize=10, \n",
        "                linewidth=2.5, color='#E63946', label='Model performance')\n",
        "        ax1.axhline(standard_loss, color='#457B9D', linestyle='--', \n",
        "                   linewidth=2, label=f'Standard: {standard_loss:.3f}')\n",
        "        ax1.axvline(1.0, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
        "        \n",
        "        ax1.set_xlabel('Input (X) Scale Factor', fontsize=12, fontweight='bold')\n",
        "        ax1.set_ylabel('Final Point Squared Error', fontsize=12, fontweight='bold')\n",
        "        ax1.set_title('Input Scaling Robustness', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xscale('log')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.legend(fontsize=10)\n",
        "    \n",
        "    # Y-scaling\n",
        "    if scaling_tests['y']:\n",
        "        y_scales = sorted(scaling_tests['y'].keys())\n",
        "        y_losses = [scaling_tests['y'][s] for s in y_scales]\n",
        "        \n",
        "        ax2.plot(y_scales, y_losses, marker='s', markersize=10, \n",
        "                linewidth=2.5, color='#F4A261', label='Model performance')\n",
        "        ax2.axhline(standard_loss, color='#457B9D', linestyle='--', \n",
        "                   linewidth=2, label=f'Standard: {standard_loss:.3f}')\n",
        "        ax2.axvline(1.0, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
        "        \n",
        "        ax2.set_xlabel('Output (Y) Scale Factor', fontsize=12, fontweight='bold')\n",
        "        ax2.set_ylabel('Final Point Squared Error', fontsize=12, fontweight='bold')\n",
        "        ax2.set_title('Output Scaling Robustness', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xscale('log')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.legend(fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print scaling analysis\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SCALING ROBUSTNESS ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    if scaling_tests['x']:\n",
        "        print(f\"\\nInput (X) Scaling:\")\n",
        "        print(f\"{'  Scale':<12} {'Loss':<12} {'vs Standard':<15}\")\n",
        "        print(f\"  {'-'*40}\")\n",
        "        for scale in sorted(scaling_tests['x'].keys()):\n",
        "            loss = scaling_tests['x'][scale]\n",
        "            diff = ((loss / standard_loss - 1) * 100) if standard_loss else 0\n",
        "            print(f\"  {scale:<12.3f} {loss:<12.4f} {diff:>+12.1f}%\")\n",
        "    \n",
        "    if scaling_tests['y']:\n",
        "        print(f\"\\nOutput (Y) Scaling:\")\n",
        "        print(f\"{'  Scale':<12} {'Loss':<12} {'vs Standard':<15}\")\n",
        "        print(f\"  {'-'*40}\")\n",
        "        for scale in sorted(scaling_tests['y'].keys()):\n",
        "            loss = scaling_tests['y'][scale]\n",
        "            diff = ((loss / standard_loss - 1) * 100) if standard_loss else 0\n",
        "            print(f\"  {scale:<12.3f} {loss:<12.4f} {diff:>+12.1f}%\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "else:\n",
        "    print(\"No scaling test results found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Complete Evaluation Summary\n",
        "\n",
        "Generate a comprehensive summary of all evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"COMPLETE EVALUATION SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f\"\\nüìã MODEL INFORMATION:\")\n",
        "print(f\"  Run ID: {run_id}\")\n",
        "print(f\"  Model Family: {conf.model.family}\")\n",
        "print(f\"  Architecture: {conf.model.n_layer} layers, {conf.model.n_head} heads, {conf.model.n_embd} embedding dim\")\n",
        "print(f\"  Task: {conf.training.task}\")\n",
        "print(f\"  Training dimensions: {conf.model.n_dims}\")\n",
        "print(f\"  Context length: {conf.model.n_positions}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
        "if \"standard\" in all_metrics and model_name in all_metrics[\"standard\"]:\n",
        "    final_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "    improvement = (1 - final_loss / n_dims) * 100\n",
        "    print(f\"  Standard (in-distribution) loss: {final_loss:.4f}\")\n",
        "    print(f\"  Improvement over baseline: {improvement:.1f}%\")\n",
        "\n",
        "print(f\"\\nüéØ BASELINE COMPARISONS:\")\n",
        "if \"standard\" in all_metrics:\n",
        "    for baseline_key in ['OLS_driver=None', 'averaging', 'NN_n=3_uniform']:\n",
        "        if baseline_key in all_metrics[\"standard\"]:\n",
        "            baseline_loss = all_metrics[\"standard\"][baseline_key][\"mean\"][-1]\n",
        "            baseline_name = baseline_names(baseline_key)\n",
        "            if model_name in all_metrics[\"standard\"]:\n",
        "                model_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "                advantage = ((baseline_loss - model_loss) / baseline_loss * 100)\n",
        "                symbol = \"‚úÖ\" if advantage > 0 else \"‚ö†Ô∏è\"\n",
        "                print(f\"  {symbol} vs {baseline_name}: {advantage:+.1f}% {'better' if advantage > 0 else 'worse'}\")\n",
        "\n",
        "print(f\"\\nüåê OUT-OF-DISTRIBUTION ROBUSTNESS:\")\n",
        "if ood_results:\n",
        "    for scenario, loss in ood_results.items():\n",
        "        clean_name = scenario.replace('_', ' ').title()\n",
        "        if standard_loss:\n",
        "            degradation = ((loss / standard_loss - 1) * 100)\n",
        "            status = \"‚úÖ\" if abs(degradation) < 15 else \"‚ö†Ô∏è\" if abs(degradation) < 30 else \"‚ùå\"\n",
        "            print(f\"  {status} {clean_name}: {loss:.4f} ({degradation:+.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ {clean_name}: {loss:.4f}\")\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  SCALING ROBUSTNESS:\")\n",
        "if scaling_tests['x'] or scaling_tests['y']:\n",
        "    if scaling_tests['x']:\n",
        "        max_x_deg = max([abs((loss / standard_loss - 1) * 100) for loss in scaling_tests['x'].values()]) if standard_loss else 0\n",
        "        print(f\"  Input (X) scaling: max degradation {max_x_deg:.1f}%\")\n",
        "    if scaling_tests['y']:\n",
        "        max_y_deg = max([abs((loss / standard_loss - 1) * 100) for loss in scaling_tests['y'].values()]) if standard_loss else 0\n",
        "        print(f\"  Output (Y) scaling: max degradation {max_y_deg:.1f}%\")\n",
        "\n",
        "print(f\"\\nüíæ SAVED RESULTS:\")\n",
        "print(f\"  Metrics file: {os.path.join(run_path, 'metrics.json')}\")\n",
        "print(f\"  Model checkpoint: {os.path.join(run_path, 'state.pt')}\")\n",
        "print(f\"  Configuration: {os.path.join(run_path, 'config.yaml')}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"‚úÖ EVALUATION COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nAll results have been cached and can be reloaded without recomputing.\")\n",
        "print(f\"To share results, download the metrics.json file from: {run_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Export/Download Results (Optional)\n",
        "\n",
        "Download evaluation results to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to download files when running on Colab\n",
        "# from google.colab import files\n",
        "\n",
        "# Download metrics\n",
        "# metrics_file = os.path.join(run_path, \"metrics.json\")\n",
        "# if os.path.exists(metrics_file):\n",
        "#     print(f\"Downloading {metrics_file}...\")\n",
        "#     files.download(metrics_file)\n",
        "#     print(\"‚úì Metrics downloaded\")\n",
        "\n",
        "# Download config\n",
        "# config_file = os.path.join(run_path, \"config.yaml\")\n",
        "# if os.path.exists(config_file):\n",
        "#     print(f\"Downloading {config_file}...\")\n",
        "#     files.download(config_file)\n",
        "#     print(\"‚úì Config downloaded\")\n",
        "\n",
        "# Create a summary report\n",
        "summary_file = os.path.join(run_path, \"evaluation_summary.txt\")\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EVALUATION SUMMARY\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(f\"Model: {conf.model.family}\\n\")\n",
        "    f.write(f\"Task: {conf.training.task}\\n\")\n",
        "    f.write(f\"Run ID: {run_id}\\n\\n\")\n",
        "    \n",
        "    if \"standard\" in all_metrics and model_name in all_metrics[\"standard\"]:\n",
        "        final_loss = all_metrics[\"standard\"][model_name][\"mean\"][-1]\n",
        "        f.write(f\"Standard Loss: {final_loss:.4f}\\n\")\n",
        "        f.write(f\"Improvement: {(1 - final_loss / n_dims) * 100:.1f}%\\n\\n\")\n",
        "    \n",
        "    f.write(\"Evaluation Scenarios:\\n\")\n",
        "    for scenario in all_metrics.keys():\n",
        "        if model_name in all_metrics[scenario]:\n",
        "            loss = all_metrics[scenario][model_name][\"mean\"][-1]\n",
        "            f.write(f\"  - {scenario}: {loss:.4f}\\n\")\n",
        "\n",
        "print(f\"\\n‚úì Summary saved to: {summary_file}\")\n",
        "\n",
        "# Uncomment to download summary\n",
        "# files.download(summary_file)\n",
        "\n",
        "print(\"\\nüí° Tip: Run this notebook anytime to re-evaluate or visualize cached results!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

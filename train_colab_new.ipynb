{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Context Learning Training on Google Colab\n",
        "\n",
        "This notebook trains transformer models for in-context learning tasks.\n",
        "\n",
        "**Setup Instructions:**\n",
        "1. Runtime → Change runtime type → GPU (T4, A100, or V100)\n",
        "2. Run cells sequentially\n",
        "3. Authenticate with Weights & Biases when prompted\n",
        "\n",
        "**Note:** This notebook uses the new YAML-based configuration system (no quinine dependency).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU and Python Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(\"Installing packages...\\n\")\n",
        "\n",
        "# Core ML packages\n",
        "%pip install -q transformers>=4.30.0\n",
        "%pip install -q wandb\n",
        "%pip install -q xgboost\n",
        "%pip install -q matplotlib seaborn tqdm\n",
        "%pip install -q pyyaml\n",
        "\n",
        "# PyTorch usually comes pre-installed in Colab\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"✓ PyTorch already installed: {torch.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch...\")\n",
        "    %pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All required packages installed successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verify key packages\n",
        "import torch\n",
        "import transformers\n",
        "import wandb\n",
        "import yaml\n",
        "\n",
        "print(f\"\\nPackage Versions:\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "print(f\"  Wandb: {wandb.__version__}\")\n",
        "\n",
        "print(f\"\\nGPU Information:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  ⚠️  No GPU detected! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
        "\n",
        "print(\"\\n✓ Ready to proceed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clone/Setup Repository\n",
        "\n",
        "Choose one of the following options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Clone from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REPO_URL = \"https://github.com/yourusername/in-context-learning.git\"  # UPDATE THIS!\n",
        "\n",
        "if not os.path.exists(\"in-context-learning\"):\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    result = subprocess.run([\"git\", \"clone\", REPO_URL], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "    else:\n",
        "        print(f\"Error cloning repository: {result.stderr}\")\n",
        "else:\n",
        "    print(\"✓ Repository already exists\")\n",
        "\n",
        "%cd in-context-learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option B: Mount Google Drive (uncomment if needed)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/in-context-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to paste your API key)\n",
        "wandb.login()\n",
        "\n",
        "print(\"✓ W&B authenticated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuration Setup\n",
        "\n",
        "Define your training configuration. This replaces the need for external config files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define your configuration\n",
        "config = {\n",
        "    'model': {\n",
        "        'family': 'gpt2',  # Options: 'gpt2' or 'lstm'\n",
        "        'n_positions': 256,  # Maximum context length\n",
        "        'n_dims': 20,  # Latent dimension\n",
        "        'n_embd': 256,  # Embedding dimension\n",
        "        'n_layer': 12,  # Number of layers\n",
        "        'n_head': 8,  # Number of attention heads\n",
        "    },\n",
        "    'training': {\n",
        "        'task': 'linear_regression',  # Task type\n",
        "        # Options: linear_regression, sparse_linear_regression, \n",
        "        #          linear_classification, relu_2nn_regression, decision_tree\n",
        "        'task_kwargs': {},  # Task-specific arguments\n",
        "        'num_tasks': None,  # Number of tasks (None = unlimited)\n",
        "        'num_training_examples': None,  # Training examples (None = unlimited)\n",
        "        'data': 'gaussian',  # Data distribution\n",
        "        'batch_size': 64,  # Batch size\n",
        "        'learning_rate': 3e-4,  # Learning rate\n",
        "        'train_steps': 10000,  # Total training steps\n",
        "        'save_every_steps': 1000,  # Checkpoint frequency\n",
        "        'keep_every_steps': -1,  # Permanent checkpoint frequency (-1 = disabled)\n",
        "        'resume_id': None,  # Resume from run ID (None = new run)\n",
        "        'curriculum': {\n",
        "            'dims': {\n",
        "                'start': 5,  # Initial dimensions\n",
        "                'end': 20,  # Final dimensions\n",
        "                'inc': 1,  # Increment per update\n",
        "                'interval': 100,  # Update every N steps\n",
        "            },\n",
        "            'points': {\n",
        "                'start': 10,  # Initial points\n",
        "                'end': 41,  # Final points\n",
        "                'inc': 1,  # Increment per update\n",
        "                'interval': 100,  # Update every N steps\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    'wandb': {\n",
        "        'project': 'in-context-training',  # W&B project name\n",
        "        'entity': 'your-entity',  # W&B entity/team name - UPDATE THIS!\n",
        "        'notes': 'Training run from Colab',  # Run notes\n",
        "        'name': None,  # Run name (None = auto-generated)\n",
        "        'log_every_steps': 10,  # Logging frequency\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save config to file\n",
        "config_path = 'train_config.yaml'\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(\"Configuration saved to:\", config_path)\n",
        "print(\"\\nConfiguration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Import Training Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "# Import required modules\n",
        "import torch\n",
        "from random import randint\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "\n",
        "from eval import get_run_metrics\n",
        "from tasks import get_task_sampler\n",
        "from samplers import get_data_sampler\n",
        "from curriculum import Curriculum\n",
        "from models import build_model\n",
        "from config import ConfigDict, validate_config, set_defaults\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(\"✓ All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Define Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(model, xs, ys, optimizer, loss_func):\n",
        "    \"\"\"Execute a single training step.\"\"\"\n",
        "    optimizer.zero_grad()\n",
        "    output = model(xs, ys)\n",
        "    loss = loss_func(output, ys)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.detach().item(), output.detach()\n",
        "\n",
        "\n",
        "def sample_seeds(total_seeds, count):\n",
        "    \"\"\"Sample random seeds for reproducible training examples.\"\"\"\n",
        "    seeds = set()\n",
        "    while len(seeds) < count:\n",
        "        seeds.add(randint(0, total_seeds - 1))\n",
        "    return seeds\n",
        "\n",
        "\n",
        "def train(model, args):\n",
        "    \"\"\"Main training loop.\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.training.learning_rate)\n",
        "    curriculum = Curriculum(args.training.curriculum)\n",
        "\n",
        "    starting_step = 0\n",
        "    state_path = os.path.join(args.out_dir, \"state.pt\")\n",
        "    if os.path.exists(state_path):\n",
        "        state = torch.load(state_path)\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "        starting_step = state[\"train_step\"]\n",
        "        for i in range(state[\"train_step\"] + 1):\n",
        "            curriculum.update()\n",
        "        print(f\"✓ Resumed from step {starting_step}\")\n",
        "\n",
        "    n_dims = model.n_dims\n",
        "    bsize = args.training.batch_size\n",
        "    data_sampler = get_data_sampler(args.training.data, n_dims=n_dims)\n",
        "    task_sampler = get_task_sampler(\n",
        "        args.training.task,\n",
        "        n_dims,\n",
        "        bsize,\n",
        "        num_tasks=args.training.num_tasks,\n",
        "        **args.training.task_kwargs,\n",
        "    )\n",
        "    pbar = tqdm(range(starting_step, args.training.train_steps))\n",
        "\n",
        "    num_training_examples = args.training.num_training_examples\n",
        "\n",
        "    for i in pbar:\n",
        "        data_sampler_args = {}\n",
        "        task_sampler_args = {}\n",
        "\n",
        "        if \"sparse\" in args.training.task:\n",
        "            task_sampler_args[\"valid_coords\"] = curriculum.n_dims_truncated\n",
        "        if num_training_examples is not None:\n",
        "            assert num_training_examples >= bsize\n",
        "            seeds = sample_seeds(num_training_examples, bsize)\n",
        "            data_sampler_args[\"seeds\"] = seeds\n",
        "            task_sampler_args[\"seeds\"] = [s + 1 for s in seeds]\n",
        "\n",
        "        xs = data_sampler.sample_xs(\n",
        "            curriculum.n_points,\n",
        "            bsize,\n",
        "            curriculum.n_dims_truncated,\n",
        "            **data_sampler_args,\n",
        "        )\n",
        "        task = task_sampler(**task_sampler_args)\n",
        "        ys = task.evaluate(xs)\n",
        "\n",
        "        loss_func = task.get_training_metric()\n",
        "\n",
        "        loss, output = train_step(model, xs.cuda(), ys.cuda(), optimizer, loss_func)\n",
        "\n",
        "        point_wise_tags = list(range(curriculum.n_points))\n",
        "        point_wise_loss_func = task.get_metric()\n",
        "        point_wise_loss = point_wise_loss_func(output, ys.cuda()).mean(dim=0)\n",
        "\n",
        "        baseline_loss = (\n",
        "            sum(\n",
        "                max(curriculum.n_dims_truncated - ii, 0)\n",
        "                for ii in range(curriculum.n_points)\n",
        "            )\n",
        "            / curriculum.n_points\n",
        "        )\n",
        "\n",
        "        if i % args.wandb.log_every_steps == 0 and not args.test_run:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"overall_loss\": loss,\n",
        "                    \"excess_loss\": loss / baseline_loss,\n",
        "                    \"pointwise/loss\": dict(\n",
        "                        zip(point_wise_tags, point_wise_loss.cpu().numpy())\n",
        "                    ),\n",
        "                    \"n_points\": curriculum.n_points,\n",
        "                    \"n_dims\": curriculum.n_dims_truncated,\n",
        "                },\n",
        "                step=i,\n",
        "            )\n",
        "\n",
        "        curriculum.update()\n",
        "\n",
        "        pbar.set_description(f\"loss {loss:.4f}\")\n",
        "        if i % args.training.save_every_steps == 0 and not args.test_run:\n",
        "            training_state = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": i,\n",
        "            }\n",
        "            torch.save(training_state, state_path)\n",
        "\n",
        "        if (\n",
        "            args.training.keep_every_steps > 0\n",
        "            and i % args.training.keep_every_steps == 0\n",
        "            and not args.test_run\n",
        "            and i > 0\n",
        "        ):\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"model_{i}.pt\"))\n",
        "\n",
        "    print(\"\\n✓ Training completed!\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "    if args.test_run:\n",
        "        curriculum_args = args.training.curriculum\n",
        "        curriculum_args['points']['start'] = curriculum_args['points']['end']\n",
        "        curriculum_args['dims']['start'] = curriculum_args['dims']['end']\n",
        "        args.training.train_steps = 100\n",
        "        print(\"Running in test mode (100 steps)\")\n",
        "    else:\n",
        "        wandb.init(\n",
        "            dir=args.out_dir,\n",
        "            project=args.wandb.project,\n",
        "            entity=args.wandb.entity,\n",
        "            config=dict(args),\n",
        "            notes=args.wandb.notes,\n",
        "            name=args.wandb.name,\n",
        "            resume=True,\n",
        "        )\n",
        "        print(f\"✓ W&B run initialized: {wandb.run.name}\")\n",
        "\n",
        "    model = build_model(args.model)\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {args.model.family}\")\n",
        "    print(f\"Task: {args.training.task}\")\n",
        "    print(f\"Training steps: {args.training.train_steps}\")\n",
        "    print(f\"Batch size: {args.training.batch_size}\")\n",
        "    print(f\"Learning rate: {args.training.learning_rate}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    train(model, args)\n",
        "\n",
        "    if not args.test_run:\n",
        "        _ = get_run_metrics(args.out_dir)  # Precompute metrics for eval\n",
        "        print(\"✓ Metrics computed\")\n",
        "\n",
        "\n",
        "print(\"✓ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = yaml.safe_load(f)\n",
        "\n",
        "# Set defaults and validate\n",
        "set_defaults(config_dict)\n",
        "validate_config(config_dict)\n",
        "\n",
        "# Add required fields\n",
        "config_dict['out_dir'] = './outputs'  # Output directory\n",
        "config_dict['test_run'] = False  # Set to True for quick test run (100 steps)\n",
        "\n",
        "# Convert to ConfigDict for attribute access\n",
        "args = ConfigDict(config_dict)\n",
        "\n",
        "# Verify model family\n",
        "assert args.model.family in [\"gpt2\", \"lstm\"], f\"Invalid model family: {args.model.family}\"\n",
        "\n",
        "# Create output directory with unique run ID\n",
        "if not args.test_run:\n",
        "    run_id = args.training.resume_id\n",
        "    if run_id is None:\n",
        "        run_id = str(uuid.uuid4())\n",
        "\n",
        "    out_dir = os.path.join(args.out_dir, run_id)\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "    args.out_dir = out_dir\n",
        "\n",
        "    # Save config to output directory\n",
        "    with open(os.path.join(out_dir, \"config.yaml\"), \"w\") as yaml_file:\n",
        "        yaml.dump(dict(args), yaml_file, default_flow_style=False)\n",
        "    \n",
        "    print(f\"✓ Output directory: {out_dir}\")\n",
        "    print(f\"✓ Run ID: {run_id}\")\n",
        "else:\n",
        "    print(\"Running in test mode (no output saved)\")\n",
        "\n",
        "print(\"\\n✓ Configuration prepared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "print(\"Starting training...\\n\")\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Monitor Training\n",
        "\n",
        "You can monitor training progress in real-time using Weights & Biases:\n",
        "- Click on the W&B run link printed above\n",
        "- View loss curves, metrics, and system stats\n",
        "- Compare with other runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save/Download Model\n",
        "\n",
        "After training completes, you can download the model checkpoints:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained model to local machine\n",
        "from google.colab import files\n",
        "\n",
        "# Download the final state\n",
        "if not args.test_run:\n",
        "    state_file = os.path.join(args.out_dir, \"state.pt\")\n",
        "    if os.path.exists(state_file):\n",
        "        print(f\"Downloading {state_file}...\")\n",
        "        files.download(state_file)\n",
        "    \n",
        "    # Also download the config\n",
        "    config_file = os.path.join(args.out_dir, \"config.yaml\")\n",
        "    if os.path.exists(config_file):\n",
        "        print(f\"Downloading {config_file}...\")\n",
        "        files.download(config_file)\n",
        "    \n",
        "    print(\"✓ Model files downloaded\")\n",
        "else:\n",
        "    print(\"Test run - no files to download\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Optional: Run Quick Test\n",
        "\n",
        "Run a quick test with just 100 steps to verify everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test run (100 steps, no logging)\n",
        "test_config = config_dict.copy()\n",
        "test_config['test_run'] = True\n",
        "test_args = ConfigDict(test_config)\n",
        "\n",
        "print(\"Running quick test (100 steps)...\\n\")\n",
        "main(test_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Options Reference\n",
        "\n",
        "### Model Configuration\n",
        "- `family`: 'gpt2' or 'lstm'\n",
        "- `n_positions`: Maximum sequence length\n",
        "- `n_dims`: Latent dimension size\n",
        "- `n_embd`: Embedding dimension\n",
        "- `n_layer`: Number of transformer/LSTM layers\n",
        "- `n_head`: Number of attention heads (for GPT-2)\n",
        "\n",
        "### Training Tasks\n",
        "Available tasks:\n",
        "1. `linear_regression`: Linear regression\n",
        "2. `sparse_linear_regression`: Sparse linear regression\n",
        "3. `linear_classification`: Linear classification\n",
        "4. `relu_2nn_regression`: 2-layer ReLU neural network regression\n",
        "5. `decision_tree`: Decision tree learning\n",
        "\n",
        "### Curriculum Learning\n",
        "- `start`: Initial value\n",
        "- `end`: Final value\n",
        "- `inc`: Increment per update\n",
        "- `interval`: Update frequency (steps)\n",
        "\n",
        "### Training Parameters\n",
        "- `batch_size`: Batch size (default: 64)\n",
        "- `learning_rate`: Learning rate (default: 3e-4)\n",
        "- `train_steps`: Total training steps\n",
        "- `save_every_steps`: Checkpoint frequency\n",
        "- `keep_every_steps`: Permanent checkpoint frequency (-1 to disable)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

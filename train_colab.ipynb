{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fKvnLWQrU_Z"
      },
      "source": [
        "# In-Context Learning Training on Google Colab\n",
        "\n",
        "This notebook trains transformer models for in-context learning tasks.\n",
        "\n",
        "**Setup Instructions:**\n",
        "1. Runtime → Change runtime type → GPU (T4, A100, or V100)\n",
        "2. Run cells sequentially\n",
        "3. Authenticate with Weights & Biases when prompted\n",
        "\n",
        "**Note:** This notebook uses the new YAML-based configuration system (no quinine dependency).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9tdvzPrU_a"
      },
      "source": [
        "## 1. Check GPU and Python Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGCl6Gf1rU_a",
        "outputId": "3dbefb73-fee6-420d-bae2-e06ba8c35ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 11 06:50:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   56C    P8             13W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfIvzMEOrU_a"
      },
      "source": [
        "## 2. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsx4QluMrU_a",
        "outputId": "f0dab0ca-57b5-4b0b-8811-554d721f61db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages...\n",
            "\n",
            "✓ PyTorch already installed: 2.8.0+cu126\n",
            "\n",
            "============================================================\n",
            "✓ All required packages installed successfully!\n",
            "============================================================\n",
            "\n",
            "Package Versions:\n",
            "  PyTorch: 2.8.0+cu126\n",
            "  Transformers: 4.57.1\n",
            "  Wandb: 0.22.3\n",
            "\n",
            "GPU Information:\n",
            "  CUDA available: True\n",
            "  CUDA version: 12.6\n",
            "  GPU: NVIDIA L4\n",
            "  GPU Memory: 23.80 GB\n",
            "\n",
            "✓ Ready to proceed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"Installing packages...\\n\")\n",
        "\n",
        "# Core ML packages\n",
        "%pip install -q transformers>=4.30.0\n",
        "%pip install -q wandb\n",
        "%pip install -q xgboost\n",
        "%pip install -q matplotlib seaborn tqdm\n",
        "%pip install -q pyyaml\n",
        "%pip install munch\n",
        "\n",
        "# PyTorch usually comes pre-installed in Colab\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"✓ PyTorch already installed: {torch.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch...\")\n",
        "    %pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All required packages installed successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verify key packages\n",
        "import torch\n",
        "import transformers\n",
        "import wandb\n",
        "import yaml\n",
        "\n",
        "print(f\"\\nPackage Versions:\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "print(f\"  Wandb: {wandb.__version__}\")\n",
        "\n",
        "print(f\"\\nGPU Information:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"  ⚠️  No GPU detected! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
        "\n",
        "print(\"\\n✓ Ready to proceed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03_krESxrU_b"
      },
      "source": [
        "## 3. Clone/Setup Repository\n",
        "\n",
        "Choose one of the following options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUjlkJrBrU_b",
        "outputId": "73f7930e-23c1-4382-fb97-d3270a61e7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository from https://github.com/hingma/in-context-learning.git...\n",
            "✓ Repository cloned successfully\n",
            "/content/in-context-learning\n"
          ]
        }
      ],
      "source": [
        "# Option A: Clone from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "REPO_URL = \"https://github.com/hingma/in-context-learning.git\"  # UPDATE THIS!\n",
        "\n",
        "if not os.path.exists(\"in-context-learning\"):\n",
        "    print(f\"Cloning repository from {REPO_URL}...\")\n",
        "    result = subprocess.run([\"git\", \"clone\", REPO_URL], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"✓ Repository cloned successfully\")\n",
        "    else:\n",
        "        print(f\"Error cloning repository: {result.stderr}\")\n",
        "else:\n",
        "    print(\"✓ Repository already exists\")\n",
        "\n",
        "%cd in-context-learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGoH3uoarU_b"
      },
      "outputs": [],
      "source": [
        "# Option B: Mount Google Drive (uncomment if needed)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/in-context-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCzHpZ50rU_b"
      },
      "source": [
        "## 4. Setup Weights & Biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggMAkHefrU_b",
        "outputId": "21bfd4f4-86c1-44cb-cc07-90d75a987926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ W&B authenticated\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to paste your API key)\n",
        "wandb.login()\n",
        "\n",
        "print(\"✓ W&B authenticated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADGtp6irU_b"
      },
      "source": [
        "## 5. Configuration Setup\n",
        "\n",
        "Define your training configuration. This replaces the need for external config files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJprfdb7rU_c",
        "outputId": "86c7e6f4-ddff-413b-bbf2-7113f08a33ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration saved to: train_config.yaml\n",
            "\n",
            "Configuration:\n",
            "model:\n",
            "  family: gpt2\n",
            "  n_dims: 20\n",
            "  n_embd: 256\n",
            "  n_head: 8\n",
            "  n_layer: 12\n",
            "  n_positions: 256\n",
            "training:\n",
            "  batch_size: 64\n",
            "  curriculum:\n",
            "    dims:\n",
            "      end: 20\n",
            "      inc: 1\n",
            "      interval: 100\n",
            "      start: 5\n",
            "    points:\n",
            "      end: 41\n",
            "      inc: 1\n",
            "      interval: 100\n",
            "      start: 10\n",
            "  data: gaussian\n",
            "  keep_every_steps: -1\n",
            "  learning_rate: 0.0003\n",
            "  num_tasks: null\n",
            "  num_training_examples: null\n",
            "  resume_id: null\n",
            "  save_every_steps: 1000\n",
            "  task: linear_regression\n",
            "  task_kwargs: {}\n",
            "  train_steps: 10000\n",
            "wandb:\n",
            "  entity: null\n",
            "  log_every_steps: 10\n",
            "  name: null\n",
            "  notes: Training run from Colab\n",
            "  project: in-context-training\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define your configuration\n",
        "config = {\n",
        "    'model': {\n",
        "        'family': 'gpt2',  # Options: 'gpt2' or 'lstm'\n",
        "        'n_positions': 256,  # Maximum context length\n",
        "        'n_dims': 20,  # Latent dimension\n",
        "        'n_embd': 256,  # Embedding dimension\n",
        "        'n_layer': 12,  # Number of layers\n",
        "        'n_head': 8,  # Number of attention heads\n",
        "    },\n",
        "    'training': {\n",
        "        'task': 'linear_regression',  # Task type\n",
        "        # Options: linear_regression, sparse_linear_regression,\n",
        "        #          linear_classification, relu_2nn_regression, decision_tree\n",
        "        'task_kwargs': {},  # Task-specific arguments\n",
        "        'num_tasks': None,  # Number of tasks (None = unlimited)\n",
        "        'num_training_examples': None,  # Training examples (None = unlimited)\n",
        "        'data': 'gaussian',  # Data distribution\n",
        "        'batch_size': 64,  # Batch size\n",
        "        'learning_rate': 3e-4,  # Learning rate\n",
        "        'train_steps': 10000,  # Total training steps\n",
        "        'save_every_steps': 1000,  # Checkpoint frequency\n",
        "        'keep_every_steps': -1,  # Permanent checkpoint frequency (-1 = disabled)\n",
        "        'resume_id': None,  # Resume from run ID (None = new run)\n",
        "        'curriculum': {\n",
        "            'dims': {\n",
        "                'start': 5,  # Initial dimensions\n",
        "                'end': 20,  # Final dimensions\n",
        "                'inc': 1,  # Increment per update\n",
        "                'interval': 100,  # Update every N steps\n",
        "            },\n",
        "            'points': {\n",
        "                'start': 10,  # Initial points\n",
        "                'end': 41,  # Final points\n",
        "                'inc': 1,  # Increment per update\n",
        "                'interval': 100,  # Update every N steps\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    'wandb': {\n",
        "    'project': 'in-context-training',\n",
        "    'entity': None,  # ← Use your default entity\n",
        "    'notes': 'Training run from Colab',\n",
        "    'name': None,\n",
        "    'log_every_steps': 10,\n",
        "    },\n",
        "    # 'wandb': {\n",
        "    #     'project': 'in-context-training',  # W&B project name\n",
        "    #     'entity': 'moxintang',  # W&B entity/team name - UPDATE THIS!\n",
        "    #     'notes': 'Training run from Colab',  # Run notes\n",
        "    #     'name': None,  # Run name (None = auto-generated)\n",
        "    #     'log_every_steps': 10,  # Logging frequency\n",
        "    # },\n",
        "}\n",
        "\n",
        "# Save config to file\n",
        "config_path = 'train_config.yaml'\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(\"Configuration saved to:\", config_path)\n",
        "print(\"\\nConfiguration:\")\n",
        "print(yaml.dump(config, default_flow_style=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__bcoYGSrU_c"
      },
      "source": [
        "## 6. Import Training Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s-092ByrU_c",
        "outputId": "1f53016a-c8ea-421e-8b8c-551c1c74e005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All modules imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "# Import required modules\n",
        "import torch\n",
        "from random import randint\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "\n",
        "from eval import get_run_metrics\n",
        "from tasks import get_task_sampler\n",
        "from samplers import get_data_sampler\n",
        "from curriculum import Curriculum\n",
        "from models import build_model\n",
        "from config import ConfigDict, validate_config, set_defaults\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(\"✓ All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSKNTDtErU_c"
      },
      "source": [
        "## 7. Define Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXdNYFgHrU_c",
        "outputId": "b5fff4ca-81bf-424c-a15d-5a4f2e245085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training functions defined\n"
          ]
        }
      ],
      "source": [
        "def train_step(model, xs, ys, optimizer, loss_func):\n",
        "    \"\"\"Execute a single training step.\"\"\"\n",
        "    optimizer.zero_grad()\n",
        "    output = model(xs, ys)\n",
        "    loss = loss_func(output, ys)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.detach().item(), output.detach()\n",
        "\n",
        "\n",
        "def sample_seeds(total_seeds, count):\n",
        "    \"\"\"Sample random seeds for reproducible training examples.\"\"\"\n",
        "    seeds = set()\n",
        "    while len(seeds) < count:\n",
        "        seeds.add(randint(0, total_seeds - 1))\n",
        "    return seeds\n",
        "\n",
        "\n",
        "def train(model, args):\n",
        "    \"\"\"Main training loop.\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.training.learning_rate)\n",
        "    curriculum = Curriculum(args.training.curriculum)\n",
        "\n",
        "    starting_step = 0\n",
        "    state_path = os.path.join(args.out_dir, \"state.pt\")\n",
        "    if os.path.exists(state_path):\n",
        "        state = torch.load(state_path)\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "        starting_step = state[\"train_step\"]\n",
        "        for i in range(state[\"train_step\"] + 1):\n",
        "            curriculum.update()\n",
        "        print(f\"✓ Resumed from step {starting_step}\")\n",
        "\n",
        "    n_dims = model.n_dims\n",
        "    bsize = args.training.batch_size\n",
        "    data_sampler = get_data_sampler(args.training.data, n_dims=n_dims)\n",
        "    task_sampler = get_task_sampler(\n",
        "        args.training.task,\n",
        "        n_dims,\n",
        "        bsize,\n",
        "        num_tasks=args.training.num_tasks,\n",
        "        **args.training.task_kwargs,\n",
        "    )\n",
        "    pbar = tqdm(range(starting_step, args.training.train_steps))\n",
        "\n",
        "    num_training_examples = args.training.num_training_examples\n",
        "\n",
        "    for i in pbar:\n",
        "        data_sampler_args = {}\n",
        "        task_sampler_args = {}\n",
        "\n",
        "        if \"sparse\" in args.training.task:\n",
        "            task_sampler_args[\"valid_coords\"] = curriculum.n_dims_truncated\n",
        "        if num_training_examples is not None:\n",
        "            assert num_training_examples >= bsize\n",
        "            seeds = sample_seeds(num_training_examples, bsize)\n",
        "            data_sampler_args[\"seeds\"] = seeds\n",
        "            task_sampler_args[\"seeds\"] = [s + 1 for s in seeds]\n",
        "\n",
        "        xs = data_sampler.sample_xs(\n",
        "            curriculum.n_points,\n",
        "            bsize,\n",
        "            curriculum.n_dims_truncated,\n",
        "            **data_sampler_args,\n",
        "        )\n",
        "        task = task_sampler(**task_sampler_args)\n",
        "        ys = task.evaluate(xs)\n",
        "\n",
        "        loss_func = task.get_training_metric()\n",
        "\n",
        "        loss, output = train_step(model, xs.cuda(), ys.cuda(), optimizer, loss_func)\n",
        "\n",
        "        point_wise_tags = list(range(curriculum.n_points))\n",
        "        point_wise_loss_func = task.get_metric()\n",
        "        point_wise_loss = point_wise_loss_func(output, ys.cuda()).mean(dim=0)\n",
        "\n",
        "        baseline_loss = (\n",
        "            sum(\n",
        "                max(curriculum.n_dims_truncated - ii, 0)\n",
        "                for ii in range(curriculum.n_points)\n",
        "            )\n",
        "            / curriculum.n_points\n",
        "        )\n",
        "\n",
        "        if i % args.wandb.log_every_steps == 0 and not args.test_run:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"overall_loss\": loss,\n",
        "                    \"excess_loss\": loss / baseline_loss,\n",
        "                    \"pointwise/loss\": dict(\n",
        "                        zip(point_wise_tags, point_wise_loss.cpu().numpy())\n",
        "                    ),\n",
        "                    \"n_points\": curriculum.n_points,\n",
        "                    \"n_dims\": curriculum.n_dims_truncated,\n",
        "                },\n",
        "                step=i,\n",
        "            )\n",
        "\n",
        "        curriculum.update()\n",
        "\n",
        "        pbar.set_description(f\"loss {loss:.4f}\")\n",
        "        if i % args.training.save_every_steps == 0 and not args.test_run:\n",
        "            training_state = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": i,\n",
        "            }\n",
        "            torch.save(training_state, state_path)\n",
        "\n",
        "        if (\n",
        "            args.training.keep_every_steps > 0\n",
        "            and i % args.training.keep_every_steps == 0\n",
        "            and not args.test_run\n",
        "            and i > 0\n",
        "        ):\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"model_{i}.pt\"))\n",
        "\n",
        "    print(\"\\n✓ Training completed!\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "    if args.test_run:\n",
        "        curriculum_args = args.training.curriculum\n",
        "        curriculum_args['points']['start'] = curriculum_args['points']['end']\n",
        "        curriculum_args['dims']['start'] = curriculum_args['dims']['end']\n",
        "        args.training.train_steps = 100\n",
        "        print(\"Running in test mode (100 steps)\")\n",
        "    else:\n",
        "        wandb.init(\n",
        "            project=args.wandb.project,\n",
        "            config=dict(args),\n",
        "            notes=args.wandb.notes,\n",
        "            name=args.wandb.name,\n",
        "        )\n",
        "        # wandb.init(\n",
        "        #     dir=args.out_dir,\n",
        "        #     project=args.wandb.project,\n",
        "        #     entity=args.wandb.entity,\n",
        "        #     config=dict(args),\n",
        "        #     notes=args.wandb.notes,\n",
        "        #     name=args.wandb.name,\n",
        "        #     resume=True,\n",
        "        # )\n",
        "        print(f\"✓ W&B run initialized: {wandb.run.name}\")\n",
        "\n",
        "    model = build_model(args.model)\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {args.model.family}\")\n",
        "    print(f\"Task: {args.training.task}\")\n",
        "    print(f\"Training steps: {args.training.train_steps}\")\n",
        "    print(f\"Batch size: {args.training.batch_size}\")\n",
        "    print(f\"Learning rate: {args.training.learning_rate}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    train(model, args)\n",
        "\n",
        "    if not args.test_run:\n",
        "        _ = get_run_metrics(args.out_dir)  # Precompute metrics for eval\n",
        "        print(\"✓ Metrics computed\")\n",
        "\n",
        "\n",
        "print(\"✓ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfLDnQg3rU_d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K6RnEPVrU_d",
        "outputId": "698a5fdc-3f5a-4206-e4d8-c29597c8eeeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Output directory: ./outputs/27bbcdfc-aa77-4072-9f50-32ac973d01c2\n",
            "✓ Run ID: 27bbcdfc-aa77-4072-9f50-32ac973d01c2\n",
            "\n",
            "✓ Configuration prepared\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = yaml.safe_load(f)\n",
        "\n",
        "# Set defaults and validate\n",
        "set_defaults(config_dict)\n",
        "validate_config(config_dict)\n",
        "\n",
        "# Add required fields\n",
        "config_dict['out_dir'] = './outputs'  # Output directory\n",
        "config_dict['test_run'] = False  # Set to True for quick test run (100 steps)\n",
        "\n",
        "# Convert to ConfigDict for attribute access\n",
        "args = ConfigDict(config_dict)\n",
        "\n",
        "# Verify model family\n",
        "assert args.model.family in [\"gpt2\", \"lstm\"], f\"Invalid model family: {args.model.family}\"\n",
        "\n",
        "# Create output directory with unique run ID\n",
        "if not args.test_run:\n",
        "    run_id = args.training.resume_id\n",
        "    if run_id is None:\n",
        "        run_id = str(uuid.uuid4())\n",
        "\n",
        "    out_dir = os.path.join(args.out_dir, run_id)\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "    args.out_dir = out_dir\n",
        "\n",
        "    # Save config to output directory\n",
        "    with open(os.path.join(out_dir, \"config.yaml\"), \"w\") as yaml_file:\n",
        "        yaml.dump(dict(args), yaml_file, default_flow_style=False)\n",
        "\n",
        "    print(f\"✓ Output directory: {out_dir}\")\n",
        "    print(f\"✓ Run ID: {run_id}\")\n",
        "else:\n",
        "    print(\"Running in test mode (no output saved)\")\n",
        "\n",
        "print(\"\\n✓ Configuration prepared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdecmyhnrU_d"
      },
      "source": [
        "## 9. Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "kgd8oxeirU_d",
        "outputId": "36e129b8-2ceb-45bd-ff6a-cffcb1564c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/in-context-learning/wandb/run-20251111_070154-amjyf345</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/moxintang-ucb/in-context-training/runs/amjyf345' target=\"_blank\">zesty-grass-1</a></strong> to <a href='https://wandb.ai/moxintang-ucb/in-context-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/moxintang-ucb/in-context-training' target=\"_blank\">https://wandb.ai/moxintang-ucb/in-context-training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/moxintang-ucb/in-context-training/runs/amjyf345' target=\"_blank\">https://wandb.ai/moxintang-ucb/in-context-training/runs/amjyf345</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ W&B run initialized: zesty-grass-1\n",
            "\n",
            "============================================================\n",
            "Model: gpt2\n",
            "Task: linear_regression\n",
            "Training steps: 10000\n",
            "Batch size: 64\n",
            "Learning rate: 0.0003\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 19.9302:  21%|██        | 2061/10000 [01:41<06:29, 20.37it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3647046295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2776047455.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'='*60}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2776047455.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mpoint_wise_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurriculum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2776047455.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, xs, ys, optimizer, loss_func)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run training\n",
        "print(\"Starting training...\\n\")\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHDGa8tdrU_d"
      },
      "source": [
        "## 10. Monitor Training\n",
        "\n",
        "You can monitor training progress in real-time using Weights & Biases:\n",
        "- Click on the W&B run link printed above\n",
        "- View loss curves, metrics, and system stats\n",
        "- Compare with other runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvyfgZu1rU_d"
      },
      "source": [
        "## 11. Save/Download Model\n",
        "\n",
        "After training completes, you can download the model checkpoints:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T40epdVirU_d"
      },
      "outputs": [],
      "source": [
        "# Download trained model to local machine\n",
        "from google.colab import files\n",
        "\n",
        "# Download the final state\n",
        "if not args.test_run:\n",
        "    state_file = os.path.join(args.out_dir, \"state.pt\")\n",
        "    if os.path.exists(state_file):\n",
        "        print(f\"Downloading {state_file}...\")\n",
        "        files.download(state_file)\n",
        "\n",
        "    # Also download the config\n",
        "    config_file = os.path.join(args.out_dir, \"config.yaml\")\n",
        "    if os.path.exists(config_file):\n",
        "        print(f\"Downloading {config_file}...\")\n",
        "        files.download(config_file)\n",
        "\n",
        "    print(\"✓ Model files downloaded\")\n",
        "else:\n",
        "    print(\"Test run - no files to download\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kns5x22yrU_e"
      },
      "source": [
        "## 12. Optional: Run Quick Test\n",
        "\n",
        "Run a quick test with just 100 steps to verify everything works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGHNeVVYrU_e"
      },
      "outputs": [],
      "source": [
        "# Quick test run (100 steps, no logging)\n",
        "test_config = config_dict.copy()\n",
        "test_config['test_run'] = True\n",
        "test_args = ConfigDict(test_config)\n",
        "\n",
        "print(\"Running quick test (100 steps)...\\n\")\n",
        "main(test_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGVA0tAsrU_e"
      },
      "source": [
        "## Configuration Options Reference\n",
        "\n",
        "### Model Configuration\n",
        "- `family`: 'gpt2' or 'lstm'\n",
        "- `n_positions`: Maximum sequence length\n",
        "- `n_dims`: Latent dimension size\n",
        "- `n_embd`: Embedding dimension\n",
        "- `n_layer`: Number of transformer/LSTM layers\n",
        "- `n_head`: Number of attention heads (for GPT-2)\n",
        "\n",
        "### Training Tasks\n",
        "Available tasks:\n",
        "1. `linear_regression`: Linear regression\n",
        "2. `sparse_linear_regression`: Sparse linear regression\n",
        "3. `linear_classification`: Linear classification\n",
        "4. `relu_2nn_regression`: 2-layer ReLU neural network regression\n",
        "5. `decision_tree`: Decision tree learning\n",
        "\n",
        "### Curriculum Learning\n",
        "- `start`: Initial value\n",
        "- `end`: Final value\n",
        "- `inc`: Increment per update\n",
        "- `interval`: Update frequency (steps)\n",
        "\n",
        "### Training Parameters\n",
        "- `batch_size`: Batch size (default: 64)\n",
        "- `learning_rate`: Learning rate (default: 3e-4)\n",
        "- `train_steps`: Total training steps\n",
        "- `save_every_steps`: Checkpoint frequency\n",
        "- `keep_every_steps`: Permanent checkpoint frequency (-1 to disable)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}